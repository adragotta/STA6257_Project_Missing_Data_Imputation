---
title: "Missing Data and Imputation - Data Science Capstone"
author: "Anna Dragotta"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

### **What is “Missing Data and Data Imputation”?**

Missing data refers to the absence of values for certain observations or
variables in a dataset. When collecting or recording data, it is not
uncommon to encounter situations where information is not available for
every individual or specific variable. Missing data can occur for
various reasons, such as non-response by participants, data entry
errors, equipment malfunctions, or simply because certain information
was not collected.

There are three main types of missing data:

**1. Missing Completely at Random (MCAR):** The missingness of data is
unrelated to both observed and unobserved variables. In other words, the
probability of data being missing is the same for all observations.

**2. Missing at Random (MAR):** The missingness is related to observed
variables but not to the values of the missing data itself. In this
case, the probability of data being missing can be predicted by other
variables in the dataset.

**3. Missing Not at Random (MNAR):** The missingness is related to the
values of the missing data itself, even after considering observed
variables. This type of missing data can introduce bias and is often
more challenging to handle.

Handling missing data is crucial in statistical analysis because it can
lead to biased or inefficient results if not addressed properly.
Researchers employ various techniques, such as imputation methods, to
estimate or replace the missing values and ensure the validity and
reliability of their analyses. Imputation involves filling in missing
values with plausible estimates based on the observed data or using
statistical models to predict missing values.

Researchers need to be aware of the nature and mechanisms of missing
data in their datasets, as the choice of imputation method depends on
these factors. Ignoring missing data or handling it incorrectly can
impact the accuracy and generalizability of study findings.

Data imputation is a technique that aims to replace the missing data
with a substitute value. This technique can be applied when removing
data from the dataset is not feasible. Due to reasons such as reduction
in size of dataset by a large extent. Reduction in this way can also
raise concerns for biases and incorrect analysis, Overall reduction can
lead to distortion in the data variables and impact of the final model
with biases. The goal is to restore the complete dataset and not lose
anymore of the data.

### Related work

The EHR has increasingly become used for data mining and analysis for a
variety of health conditions. Although due to irregular observation
times and innate uncertainties in a medical setting the EHR data sets
are missing values. The EHR systems were not created in mind for
research. Researchers who do use this data may categorize this missing
data as missing completely at random, missing at random or not missing
at random. Regarding missing completely at random all of the data points
have the same odds to be missing. Missing at random is attributable to
something like the patient's condition, which means the missingness of
the data is independent from the value of the data. Not missing at
random data completely depends on the value of the data. For each of
these categories the level of bias can be different.

### Methods

 Some of the data imputation techniques are mentioned below.\
\
-   Next or Previous Value 

\-   K Nearest Neighbors

\-   Maximum or Minimum Value

\-   Missing Value Prediction

\-   Most Frequent Value

\-   Average or Linear Interpolation

\-   (Rounded) Mean or Moving Average or Median Value

\-   Fixed Value

\-   Rule-based imputation technique

\-   Random forest

Some imputation techniques are complete case analysis (CCA), arbitrary
value imputation and frequent category imputation. complete case
analysis consists of directly removing the rows that have missing data.
This is also known as listwise deletion. The assumption is the data is
missing at random (MAR). Arbitrary value imputation consists of handling
both numerical and categorical variables This technique also includes
grouping the missing values in a column and then a value far from the
range of the column. The values could be 999999 or -999999,  "missing"
or "not defined." The assumption for this technique is the data is not
missing at random. Frequent category imputation is a technique that
replaces the missing value with the variable that has the highest
frequency, meaning you replace the value with the mode of the column.
This is also called mode imputation. The assumptions regarding this
technique is that the data is missing at random.

Multiple imputation is another approach to missing data and is applied
to a number of common statistical packages. Multiple imputation is used
to allow for the uncertainty of the missing data via creating many
different plausible imputed datasets and appropriate mixing results
produced from each of them. The first step is to create multiple copies
of the dataset with the missing values substituted via the imputed
values.  Because one can never know the true values of the missing data,
this imputation course of action should capture fully all uncertainty in
foreseeing the missing values by inserting appropriate variability into
the multiple imputed values. The second stage is to utilize a standard
statistical method. This method should fit the model of interest for
each of the datasets that experienced imputation.

## Analysis and Results

### Data and Visualization

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Statistical Modeling

```{r}

```

### Conclusion

## References
