---
title: "Missing Data and Imputation - Data Science Capstone"
author: "Anna Dragotta"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

### What is "method"?

This is an introduction to missing data and imputation, Missing data is common in all studies to conduct primary care research. Primary care research being the research done by the members of the primary care team with the patients and communities in which they serve. Missing data is an issue because it can compromise validity of the study findings which increases risks for bias regarding subgroups of the population who may be underrepresented. There's also loss of information and reduced statistical power. Studies have shown that high rates of missing data have negative consequences such as impacting primary care study conclusions. Holistic approaches for addressing this issue would be during the design phase, conduct phase and analytic phase. Prior to study (during the design phase) a tactic could be identifying target patient populations who are under-served by intervention and may have incentives to stay in the study. Regarding conduct phase one could consider multiple methods of assessment to ensure patient can get assessed for study measures, this means making it available to do virtual, at-home or self-administered surveys. After the study (during the analysis phase there are 4 common types of methods to address missing data. This includes complete case analyses, single imputation, inverse probability weighting and multiple imputation. 





### Related work

The EHR has increasingly become used for data mining and analysis for a variety of health conditions. Although due to irregular observation times and innate uncertainties in a medical setting the EHR data sets are missing values. The EHR systems were not created in mind for research.  Researchers who do use this data may categorize this missing data as missing completely at random, missing at random or not missing at random.  Regarding missing completely at random all of the data points have the same odds to be missing. Missing at random is attributable to something like the patient's condition, which means the missingness of the data is independent from the value of the data. Not missing at random data  completely depends on the value of the data. For each of these categories the level of bias can be different.

Missing data can compromise inferences and cause bias regarding clinical trials.  
The power of a randomized clinical trials is from the random allocation of patients. Randomized clinical trials are often analyzed using the intention to treat principle. When some patients are lost to follow-up that impacts results of trial. It’s also important what is causing the missingness. There are 3 typical mechanisms that cause missing data: missing completely at random (MCAR), missing at random (MAR) and missing not at random (MNAR). To prevent missing data a randomized trial should be planned every detail in advance. Some of these details to prevent missing data include making sure registration numbers and values of stratification variables are registered. Methods to handle missing data in primary analysis is complete case analysis, single imputation, model-based methods and multiple imputation. 

Missing completely at random (MCAR) is when the data that is missing is independent of the observed and unobserved data. There are no systematic differences between patients who have the data completed versus those who don’t. An example of this could be when some patients have missing lab values because the samples were mishandled. Statistical power is decreased and the analyzable population of the study, but bias is not introduced. Missing at random (MAR) is systematically related to the observed data but not the unobserved data. This can be seen if there a survey examining depression and the male patients are less likely to compete the depression severity compared to female patients. Missing not at random (MNAR) is when the missing data is systemically related to unobserved data. This unobserved data is cannot measured by the investigator. An example of this would be if patients who have severe depression are more likely to refuse to finish the survey regarding depression severity.  

Data imputation is a technique that aims to replace the missing data with a substitute value. This technique can be applied when removing data from the dataset is not feasible. Due to reasons such as reduction in size of dataset by a large extent. Reduction in this way can also raise concerns for biases and incorrect analysis, Overall reduction can lead to distortion in the data variables and impact of the final model with biases. The goal is to restore the complete dataset and not lose anymore of the data.

Some imputation techniques are complete case analysis (CCA), arbitrary value imputation and frequent category imputation. complete case analysis consists of directly removing the rows that have missing data. This is also known as listwise deletion. The assumption is the data is missing at random (MAR). Arbitrary value imputation consists of handling both numerical and categorical variables This technique also includes grouping the missing values in a column and then a value far from the range of the column. The values could be 999999 or -999999,  "missing" or "not defined." The assumption for this technique is the data is not missing at random. Frequent category imputation is a technique that replaces the missing value with the variable that has the highest frequency, meaning you replace the value with the mode of the column. This is also called mode imputation. The assumptions regarding this technique is that the data is missing at random.

Multiple imputation is another approach to missing data and is applied to a number of common statistical packages. Multiple imputation is used to allow for the uncertainty of the missing data via creating many different plausible imputed datasets and appropriate mixing results produced from each of them. The first step is to create multiple copies of the dataset with the missing values substituted via the imputed values.  Because one can never know the true values of the missing data, this imputation course of action should capture fully all uncertainty in foreseeing the missing values by inserting appropriate variability into the multiple imputed values. The second stage is to utilize a standard statistical method. This method should fit the model of interest for each of the datasets that experienced imputation.

## Methods

The common non-parametric regression model is
$Y_i = m(X_i) + \varepsilon_i$, where $Y_i$ can be defined as the sum of
the regression function value $m(x)$ for $X_i$. Here $m(x)$ is unknown
and $\varepsilon_i$ some errors. With the help of this definition, we
can create the estimation for local averaging i.e. $m(x)$ can be
estimated with the product of $Y_i$ average and $X_i$ is near to $x$. In
other words, this means that we are discovering the line through the
data points with the help of surrounding data points. The estimation
formula is printed below [@R-base]:

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$ $W_n(x)$ is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if $X_i$ is far from $x$.

Another equation:

$$
y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i
$$

## Analysis and Results

### Data and Visualization

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Statistical Modeling

```{r}

```

### Conclusion

## References
